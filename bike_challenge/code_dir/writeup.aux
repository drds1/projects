\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction and Data Ingestion}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Data Ingestion}{1}{section.2}}
\newlabel{sec_ingest}{{2}{1}{Data Ingestion}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Exploratory Data Analysis}{1}{section.3}}
\newlabel{sec_exp}{{3}{1}{Exploratory Data Analysis}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Time Series}{1}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Time series of bike hires. Y axis plots the number of bikes hired per day as a function of day number (days are measured relative to 1st January 2014). The vertical red dashed lines show 4th September of each year (the start of the forecast week) and the smooth vertical polynomial fit shows how the average daily number of bike hires increases over the four years of data.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_ts}{{1}{2}{Time series of bike hires. Y axis plots the number of bikes hired per day as a function of day number (days are measured relative to 1st January 2014). The vertical red dashed lines show 4th September of each year (the start of the forecast week) and the smooth vertical polynomial fit shows how the average daily number of bike hires increases over the four years of data.\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Same as Figure \ref  {fig_ts} but restricting the time series only to bike hires departing at station xx and ending at station xx\relax }}{3}{figure.caption.2}}
\newlabel{fig_ts_station}{{2}{3}{Same as Figure \ref {fig_ts} but restricting the time series only to bike hires departing at station xx and ending at station xx\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Power spectrum $P(f)$ versus frequency of the time series data presented in Figure \ref  {fig_ts}. The peaks at one year and one week timescales (red dashed lines) show that bike hire goes through seasonal and weekly cycles of popularity.\relax }}{4}{figure.caption.3}}
\newlabel{fig_ps}{{3}{4}{Power spectrum $P(f)$ versus frequency of the time series data presented in Figure \ref {fig_ts}. The peaks at one year and one week timescales (red dashed lines) show that bike hire goes through seasonal and weekly cycles of popularity.\relax }{figure.caption.3}{}}
\newlabel{eq_ft}{{1}{4}{Time Series}{equation.3.1}{}}
\newlabel{eq_ps}{{2}{4}{Time Series}{equation.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Model fitting (Recurrent Neural Network)}{4}{section.4}}
\newlabel{sec_recur}{{4}{4}{Model fitting (Recurrent Neural Network)}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Mulit-Layer-Perceptron (MLP)}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Theory}{5}{subsection.4.2}}
\newlabel{eq_sigmoid}{{3}{5}{Theory}{equation.4.3}{}}
\newlabel{eq_sigmoid}{{4}{5}{Theory}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}optimizing the weights}{5}{subsection.4.3}}
\newlabel{eq_cost}{{5}{5}{optimizing the weights}{equation.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example neural network with a single neuron. The network has three inputs to neuron $j$ denoted by $X_{ij}$ with weights $W_{ij}$. The output from the neuron is given by $a_j$ where the activation function $f(z)$ is given in Equation \ref  {eq_sigmoid}.\relax }}{6}{figure.caption.4}}
\newlabel{fig_temp}{{4}{6}{Example neural network with a single neuron. The network has three inputs to neuron $j$ denoted by $X_{ij}$ with weights $W_{ij}$. The output from the neuron is given by $a_j$ where the activation function $f(z)$ is given in Equation \ref {eq_sigmoid}.\relax }{figure.caption.4}{}}
\newlabel{eq_adjust_w}{{6}{6}{optimizing the weights}{equation.4.6}{}}
\newlabel{eq_partial_tot}{{7}{6}{optimizing the weights}{equation.4.7}{}}
\newlabel{eq_partdiv}{{8}{6}{optimizing the weights}{equation.4.8}{}}
\newlabel{eq_delta}{{9}{7}{optimizing the weights}{equation.4.9}{}}
\newlabel{eq_delta_in}{{10}{7}{optimizing the weights}{equation.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Recurrent Neural Network}{7}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{7}{section.5}}
\newlabel{sec_results}{{5}{7}{Results}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Diagram showing architecture of an RNN. Rather than side by side as with a MLP, each neuron appears sequentially. Additionally an RNN produces an output $y$ at each neuron rather than just a single output at the end of the network. The activation function $f$ takes the same form as Equation \ref  {eq_sigmoid} used in the MLP, and the weights $W$ are updated using a back propagation algorithm similar to Equation \ref  {eq_adjust_w}.\relax }}{8}{figure.caption.5}}
\newlabel{fig_rnn}{{5}{8}{Diagram showing architecture of an RNN. Rather than side by side as with a MLP, each neuron appears sequentially. Additionally an RNN produces an output $y$ at each neuron rather than just a single output at the end of the network. The activation function $f$ takes the same form as Equation \ref {eq_sigmoid} used in the MLP, and the weights $W$ are updated using a back propagation algorithm similar to Equation \ref {eq_adjust_w}.\relax }{figure.caption.5}{}}
