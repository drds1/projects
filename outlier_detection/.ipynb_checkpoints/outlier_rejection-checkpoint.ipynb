{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aims to apply sigma-clipping to reject outlying data points in time-series data. The model iterativelty fits a smooth polynomial function (with the order as an input parameter) and rejects outliers beyond n-standard deviations of the fit (n is the second input parameter). The iterations end when no further poins are rejected or after a maximum of 100 is reached (the process should never take this long and a flag will raise if this occurs).\n",
    "\n",
    "First import requried modiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.optimize as mcf\n",
    "import scipy.signal as ssig\n",
    "import matplotlib.gridspec as gridspec\n",
    "from statsmodels import robust\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the outlier rejection function. \n",
    "\n",
    "\n",
    "Arguments are as follows...\n",
    "\n",
    "INPUTS: \n",
    "data_y_in, data_x_in --> 1D array containing the values of the time-series.\n",
    "\n",
    "\n",
    "\n",
    "OPTIONAL INPUTS:\n",
    "\n",
    "data_x_in            --> 1D arrays containing the time axis of the time-series (if no data_x_in entered, code assumes evenly spaced data).\n",
    "\n",
    "\n",
    "sd_check             --> The number of standard deviations from the fitted model (see fname) to consider a point outliying.\n",
    "\n",
    "fname                --> The type of smooth model to fit to the time series prior to rejection (linear, polynomial, running mean, running median).\n",
    "\n",
    "filter_size          --> The size of the window to use for running mean and median computation (for running mean and median functions only).\n",
    "\n",
    "max_iteration        --> Defines the stopping point for the sigma clipping (either when no further points are rejected or when max_iteration is reached).\n",
    "\n",
    "\n",
    "diagnostic_figure    --> The name of an output figure showing the time-series with the outliers flagged. If blank ('') then no plot is made, else enter something like 'output_figure.pdf'\n",
    "\n",
    "OUTPUTS:\n",
    "data_x               --> The output time-stamps (will be the indicees of accepted points if no input provided).\n",
    "\n",
    "data_y               --> The y-axis of accepted points in the time-series.\n",
    "\n",
    "idx_out              --> The indicees of rejected points of the input time-series (data_y_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outrej(data_y_in,data_x_in=[],sd_check=4,fname='running median',filter_size = 5,max_iteration = 10,\n",
    "           diagnostic_figure='',comments=1,spread_function = 'rms'):\n",
    "\n",
    "\n",
    " def func1(x,p0,p1):\n",
    "  return(p0+x*p1)\n",
    " def func2(x,p0,p1,p2):\n",
    "  return(p0+p1*x+p2*x**2)\n",
    " \n",
    " def movingaverage (x,y, window):\n",
    "  weights = np.repeat(1.0, window)/window\n",
    "  sma = np.convolve(y, weights, 'valid')\n",
    "  sma_x = x[:]\n",
    "  sma_y = np.concatenate((np.ones(window-1)*sma[0],sma))\n",
    "  return(sma_x,sma_y)\n",
    " \n",
    " \n",
    " #if time values not provided, assume even sampling.\n",
    " if (type(data_x_in) == np.ndarray):\n",
    "  data_x = np.array(data_x_in)\n",
    " else:\n",
    "  ndat   = np.shape(data_y_in)[0]\n",
    "  data_x = np.arange(ndat)\n",
    " data_x_in_plot = np.array(data_x)   \n",
    "\n",
    " data_y = np.array(data_y_in)\n",
    " out_x = []\n",
    " out_y = []\n",
    " idx_out = []\n",
    "    \n",
    " #perform the iterations until max_iteration or no further rejections made   \n",
    " for iteration in range(max_iteration):\n",
    "  \n",
    "  #fit smooth function (linear, polynomial, running mean, running median, global median or global mean)\n",
    "  if (fname == 'linear'): \n",
    "   popt, pcov = mcf.curve_fit(func1, data_x, data_y)\n",
    "   model_y = popt[0] + data_x*popt[1]\n",
    "   model_x = np.array(data_x)\n",
    "  elif (fname == 'quadratic'):\n",
    "   popt, pcov = mcf.curve_fit(func2, data_x, data_y)\n",
    "   model_y = popt[0] + popt[1]*data_x + popt[2]*data_x**2\n",
    "   model_x = np.array(data_x) \n",
    "  elif (fname == 'running median'):\n",
    "   model_y = ssig.medfilt(data_y, kernel_size=filter_size)\n",
    "   model_x = np.array(data_x)\n",
    "  elif (fname == 'global median'):\n",
    "   a =  np.median(data_y)\n",
    "   ndy = np.shape(data_y)[0]\n",
    "   model_y = a*np.ones(ndy)\n",
    "   model_x = np.array(data_x)\n",
    "  elif (fname == 'running mean'):\n",
    "   model_x,model_y = movingaverage(data_x,data_y,filter_size)\n",
    "  elif (fname == 'global mean'):\n",
    "   a =  np.mean(data_y)\n",
    "   ndy = np.shape(data_y)[0]\n",
    "   model_y = a*np.ones(ndy)\n",
    "   model_x = np.array(data_x)\n",
    " \n",
    "  #compute the residuals ( abs[data-model])\n",
    "  #model_itp = np.interpolate(data_x,model_x,model_y)   \n",
    "  residual = data_y - model_y\n",
    " \n",
    "     \n",
    "  #identify points greater than sd_outlier from the model \n",
    "  if (spread_function == 'rms'):\n",
    "   sd = np.std(residual)\n",
    "  elif (spread_function == 'mad'):\n",
    "   sd = robust.mad(residual)\n",
    "  else:\n",
    "   raise ValueError('Please ensure that the \"spread_function\" argument is set to \"rms\" or \"mad\"')\n",
    "  \n",
    "  id_out = np.where(np.abs(residual) > sd_check*sd)[0]\n",
    "  n_out = np.shape(id_out)[0] \n",
    " \n",
    "     \n",
    "  #flag outliers and remove from them from the data arrays for the next iteration\n",
    "  #for id in id_out:\n",
    "  # print data_x[id],data_y[id]\n",
    " \n",
    "  out_x.append(data_x[id_out])\n",
    "  out_y.append(data_y[id_out])\n",
    "  data_x = np.delete(data_x,id_out)\n",
    "  data_y = np.delete(data_y,id_out)\n",
    " \n",
    "  #save a record of rejected array indicees \n",
    "  idx_out.append(id_out)\n",
    "\n",
    "  #exit the loop prematurely if no outliers found\n",
    "  if (np.shape(id_out)[0] == 0):\n",
    "   if (comments == 1):\n",
    "    print 'no further oultiers found after iteration ',iteration,' exiting...'\n",
    "   break\n",
    "  else:\n",
    "   if (comments == 1):\n",
    "    print 'iteration ',iteration,'\\n ',n_out,'outliers found'\n",
    "    \n",
    " #flag a warning if the outlier rejection has not converged after max_iteration iterations\n",
    " if ((iteration == max_iteration - 1) and (comments == 1)):\n",
    "  print 'warning: Outlier rejection did not converge after a maximum,', max_iteration,\\\n",
    " ' iterations re-run for more iterations or check input data for bugs'\n",
    " \n",
    "    \n",
    "    \n",
    " idx_out = np.array(np.concatenate(idx_out) ,dtype='int')  \n",
    " #plot a diagnostic plot if requested\n",
    " if (diagnostic_figure != ''):\n",
    "  \n",
    "  gs1 = gridspec.GridSpec(4, 5)\n",
    "  gs1.update(left=0.1, right=0.9, bottom=0.1,top = 0.9, wspace=0.05,hspace = 0.0)\n",
    "  ax1 = plt.subplot(gs1[:3, :4])\n",
    "  ax1.plot(data_x,data_y,label='Time series',color='k')\n",
    "  ax1.plot(model_x,model_y,label='Smooth model',color='blue')\n",
    "  ax1.scatter(data_x_in_plot[idx_out],data_y_in[idx_out],marker='o',color='red',label='Outliers')\n",
    "  plt.legend()\n",
    "  ax1.set_xlabel('Time')\n",
    "  ax1.set_ylabel('Time-series values')\n",
    "  \n",
    "  axres = plt.subplot(gs1[3:, :4])\n",
    "  axres.plot(model_x,residual)\n",
    "  axres.set_xlabel('Time')\n",
    "  xl = list(axres.get_xlim())\n",
    "  axres.set_xlim(xl)\n",
    "  ax1.set_xlim(xl)\n",
    "  axres.plot(xl,[0,0],ls=':')\n",
    "  axres.set_ylabel('residuals \\n (data - model)') \n",
    "\n",
    "  axhist = plt.subplot(gs1[:3, 4])\n",
    "  axhist.hist(data_y,orientation='horizontal',color='k',histtype='step')\n",
    "  axhist.set_xticklabels([])\n",
    "  axhist.set_yticklabels([])\n",
    "  xy = list(ax1.get_ylim())\n",
    "  axhist.set_ylim(xy)\n",
    "\n",
    "  if (diagnostic_figure == 'show'):\n",
    "   plt.show()\n",
    "  else:\n",
    "   plt.savefig(diagnostic_figure)\n",
    "  plt.clf()\n",
    "    \n",
    " return(data_y,data_x,model_y,idx_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set 'demo_single_timeseries = 1' for a demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_single_timeseries = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (demo_single_timeseries == 1):\n",
    " #generate some fake random data to test the code. Specify the parameters of the fake data below\n",
    " filter_size = 5\n",
    " fname = 'running median'\n",
    " sd_fake = 3.0\n",
    " n_fake  = 1000\n",
    " \n",
    " #some subset of the fake data will be the 'test outliers' the code is tasked to identify.\n",
    " #the parameters should be setup so that n_outlier < n_fake and sd_outlier > sd_fake.\n",
    " n_outlier = 23\n",
    " sd_outlier = 10.0\n",
    " \n",
    " data_y = np.random.randn(n_fake)*sd_fake\n",
    " id_test = np.random.choice(np.arange(n_fake), size=n_outlier, replace=False)\n",
    " data_y[id_test] = np.random.randn(n_outlier)*sd_outlier\n",
    " \n",
    " #also have the option of including irregularly spaced time-series data by speciffying a \n",
    " #unique input array for the x-axis (code assumes regular sampling if no input)\n",
    " data_x = np.arange(n_fake)\n",
    " \n",
    " #Call the outlier rejection function defined above and test on the fake data.\n",
    " data_y_pass,data_x_pass,model_y,idx_outlier = outrej(data_y,data_x,sd_check=3.5,\n",
    " fname='running median',filter_size = 5,max_iteration=10,diagnostic_figure='show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is appropriate for identifying outliers from a single time series. With a few adaptions the function can allow for multiple time-series input.\n",
    "\n",
    "The function below is apporpriate for cross-checking simultaneous epochs in adjacent time-series data using the sigma-clip routine in parallel epoch-by-epoch. This is useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now define the parallel outlier rejection sigma clip\n",
    "def outrej_parallel(data_y,sd_check=5,fname='running median',filter_size = 5,max_iteration=10,diagnostic_figure='',\n",
    "                    spread_function = 'rms'):\n",
    " \n",
    "\n",
    "\n",
    " n_epoch,n_timeseries = np.shape(data_y)\n",
    "\n",
    " #now compute the sigma-clip routine across parallel time series one time at a time\n",
    " id_outliers = np.zeros((2,0),dtype='int')\n",
    " for i in range(n_epoch):\n",
    "  y_now = data_y[i,:]\n",
    "  y_pass,x_pass,model,idx_outlier = outrej(y_now,sd_check=sd_check,fname=fname,\n",
    "  filter_size = filter_size,max_iteration=max_iteration,diagnostic_figure='',\n",
    "  comments=0,spread_function = spread_function)\n",
    "  \n",
    "  #save the points identified as outliers in a 2d array with 0th column corresponding to\n",
    "  #time series ID and 1st column to the epoch id\n",
    "  n_outliers = np.shape(idx_outlier)\n",
    "  id_out = np.vstack( (np.ones(n_outliers,dtype='int')*i,idx_outlier) )\n",
    "  id_outliers = np.hstack((id_outliers,id_out))\n",
    " id_outliers = id_outliers.T[:,[1,0]]\n",
    " \n",
    " \n",
    " #plot the results\n",
    " gs1 = gridspec.GridSpec(4, 4)\n",
    " gs1.update(left=0.1, right=0.9, bottom=0.1,top = 0.9, wspace=0.05,hspace = 0.0)\n",
    " ax1 = plt.subplot(gs1[:, :])\n",
    " for i in range(n_timeseries):\n",
    "  if (i == 0):\n",
    "   labts = 'Time series'\n",
    "   labo  = 'Outliers'\n",
    "  else:\n",
    "   labts = None\n",
    "   labo  = None\n",
    "  ax1.plot(data_y[:,i],label=labts,color='k')\n",
    "  id_ts = np.where(id_outliers[:,0] == i)[0]\n",
    "  ax1.scatter(id_outliers[id_ts,1],data_y[id_outliers[id_ts,1],id_outliers[id_ts,0]],color='r',label=labo)\n",
    " plt.legend()\n",
    " ax1.set_title(np.str(n_timeseries)+' timeseries, ' + np.str(n_epoch) + ' epochs per series')\n",
    " ax1.set_xlabel('Time')\n",
    " ax1.set_ylabel('Time-series values')\n",
    " \n",
    " if (diagnostic_figure == 'show'):\n",
    "  plt.show()\n",
    " else:\n",
    "  plt.savefig(diagnostic_figure)\n",
    "  \n",
    "  \n",
    "  \n",
    " return(id_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstration below generates 'n_timeseries' sets of timeseries data, each with 'n_epoch' points. Anomalous variability is introduced in 'id_outlier' timeseries to simulate rogue outlying points. And the 'outrej_parallel' routine is asked to identify these.\n",
    "\n",
    "Set 'demo_multi_timeseries = 1' for a demonstration of the parallel sigma clip routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_multi_timeseries = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (demo_multi_timeseries == 1):\n",
    " sd_background = 3.0\n",
    " n_epoch  = 1000\n",
    " n_timeseries = 100\n",
    " id_outlier = 23\n",
    " time_anomaly = 350\n",
    " grad_anomaly = 0.1\n",
    " diagnostic_figure = 'show'\n",
    " \n",
    " \n",
    " \n",
    " data_y = np.reshape( np.random.randn(n_epoch * n_timeseries), (n_epoch,n_timeseries) )\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " #introduce a linear gradient into one timeseries to simulate non-stationarity\n",
    " data_y[time_anomaly:,id_outlier] += data_y[time_anomaly,id_outlier] + np.arange(0,n_epoch-time_anomaly,1)*grad_anomaly\n",
    " \n",
    " \n",
    " #test the parallel outlier rejection routine here\n",
    " op = outrej_parallel(data_y,sd_check=5,fname='running median',filter_size = 5,max_iteration=10,\n",
    "                      diagnostic_figure='show',spread_function = 'rms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final function to perform the outrej formulation in series rather than parallel. This is the same as the first function 'outrej' but accepts simultaneous multi light curve input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now define the parallel outlier rejection sigma clip\n",
    "def outrej_series(data_y,sd_check=5,fname='running median',filter_size = 5,max_iteration=10,diagnostic_figure='',\n",
    "                 spread_function = 'rms'):\n",
    " \n",
    "\n",
    "\n",
    " n_epoch,n_timeseries = np.shape(data_y)\n",
    "\n",
    " #now compute the sigma-clip routine across each time series one time at a time\n",
    " id_outliers = np.zeros((2,0),dtype='int')\n",
    " modelsave = []\n",
    " for i in range(n_timeseries):\n",
    "  y_now = data_y[:,i]\n",
    "  y_pass,x_pass,model,idx_outlier = outrej(y_now,sd_check=sd_check,fname=fname,\n",
    "  filter_size = filter_size,max_iteration=max_iteration,diagnostic_figure='',\n",
    "  comments=0,spread_function = spread_function)\n",
    "  \n",
    "  #save the smooth model\n",
    "  modelsave.append(model)\n",
    "  #save the points identified as outliers in a 2d array with 0th column corresponding to\n",
    "  #time series ID and 1st column to the epoch id\n",
    "  n_outliers = np.shape(idx_outlier)\n",
    "  id_out = np.vstack( (np.ones(n_outliers,dtype='int')*i,idx_outlier) )\n",
    "  id_outliers = np.hstack((id_outliers,id_out))\n",
    " id_outliers = id_outliers.T[:,[1,0]]\n",
    " \n",
    " \n",
    " #plot the results\n",
    " gs1 = gridspec.GridSpec(4, 4)\n",
    " gs1.update(left=0.1, right=0.9, bottom=0.1,top = 0.9, wspace=0.05,hspace = 0.0)\n",
    " ax1 = plt.subplot(gs1[:, :])\n",
    " for i in range(n_timeseries):\n",
    "  if (i == 0):\n",
    "   labts = 'Time series'\n",
    "   labo  = 'Outliers'\n",
    "   labmod = 'Smooth model'\n",
    "  else:\n",
    "   labts = None\n",
    "   labo  = None\n",
    "   labmod = None\n",
    "  ax1.plot(data_y[:,i],label=labts,color='k')\n",
    "  ax1.plot(modelsave[i],color='b',label=labmod)\n",
    "  id_ts = np.where(id_outliers[:,0] == i)[0]\n",
    "  ax1.scatter(id_outliers[id_ts,1],data_y[id_outliers[id_ts,1],id_outliers[id_ts,0]],color='r',label=labo)\n",
    " plt.legend()\n",
    " ax1.set_title(np.str(n_timeseries)+' timeseries, ' + np.str(n_epoch) + ' epochs per series')\n",
    " ax1.set_xlabel('Time')\n",
    " ax1.set_ylabel('Time-series values')\n",
    " \n",
    " if (diagnostic_figure == 'show'):\n",
    "  plt.show()\n",
    " else:\n",
    "  plt.savefig(diagnostic_figure)\n",
    "  \n",
    "  \n",
    "  \n",
    " return(id_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to use EVD and feature space representation of multivariate timeseries to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kdestats\n",
    "\n",
    "#rolling mean function\n",
    "def movingaverage(x,y, window):\n",
    "  weights = np.repeat(1.0, window)/window\n",
    "  sma = np.convolve(y, weights, 'valid')\n",
    "  sma_x = x[:]\n",
    "  sma_y = np.concatenate((np.ones(window-1)*sma[0],sma))\n",
    "  return(sma_x,sma_y)\n",
    "\n",
    "#calculate each of the features\n",
    "def features(data_y,window = 10,plots='show'):\n",
    "    nfeatures = 9\n",
    "    \n",
    "    nepoch,nts = np.shape(data_y)\n",
    "    features = np.zeros((nts,nfeatures))\n",
    "    \n",
    "    #f1 mean\n",
    "    f1 = np.mean(data_y,axis=0)\n",
    "    \n",
    "    \n",
    "    #f2 variance\n",
    "    f2 = np.var(data_y,axis=0)\n",
    "    \n",
    "    \n",
    "    #f3 lumpiness devide into chunks defined by window size and calculate the variance of the variances\n",
    "    nwindow = np.int(np.ceil(1.*nepoch/window))\n",
    "    lump = np.zeros((0,nts))\n",
    "    for i in range(nwindow):\n",
    "     ilo = i*window\n",
    "     ihi = min(ilo+window,nepoch-1)\n",
    "     var = np.var(data_y[ilo:ihi,:],axis=0)\n",
    "     lump = np.vstack((lump,var))\n",
    "    f3 = np.var(lump,axis=0)\n",
    "    \n",
    "    #f4 A function to calculate Level shift using rolling window The 'level shift' is defined as the maximum difference in mean\n",
    "    # between consecutive blocks of 10 observations measure6 - Level shift using rolling window\n",
    "    f4 = []\n",
    "    for i in range(nts):\n",
    "     runmean = movingaverage(np.arange(nepoch),data_y[:,i], window)[1]\n",
    "     d_runmean = runmean[1:]-runmean[:-1]\n",
    "     f4.append(np.max(d_runmean))\n",
    "    f4 = np.array(f4)\n",
    "\n",
    "    #f5 calculate the rolling variance\n",
    "    import pandas as pd \n",
    "    f5 = []\n",
    "    for i in range(nts):\n",
    "     s = pd.Series(data_y[:,i])\n",
    "     run_var = np.array(s.rolling(window).var())\n",
    "     d_runvar = run_var[1:]-run_var[:-1]\n",
    "     f5.append(np.nanmax(d_runvar))\n",
    "    f5 = np.array(f5)\n",
    "    \n",
    "    \n",
    "    #f6 fano factor (burstiness of time series - var/mean)\n",
    "    f6 = f2/f1\n",
    "    \n",
    "    #f7, f8 maximum and minimum of time series\n",
    "    f7 = np.nanmax(data_y,axis=0)\n",
    "    f8 = np.nanmin(data_y,axis=0)\n",
    "    \n",
    "    #f9 high to low mu ratio of means of the data that are above and below the true mean\n",
    "    f9 = []\n",
    "    for i in range(nts):\n",
    "        mean = f1[i]\n",
    "        idup = np.where(data_y[:,i]>mean)[0]\n",
    "        meanhi = np.mean(data_y[idup,i])\n",
    "        idlo = np.where(data_y[:,i]<mean)[0]\n",
    "        meanlo = np.mean(data_y[idlo,i])\n",
    "        f9.append(meanhi/meanlo)    \n",
    "    f9 = np.array(f9)\n",
    "    \n",
    "    features[:,0]=f1\n",
    "    features[:,1]=f2\n",
    "    features[:,2]=f3\n",
    "    features[:,3]=f4\n",
    "    features[:,4]=f5\n",
    "    features[:,5]=f6\n",
    "    features[:,6]=f7\n",
    "    features[:,7]=f8\n",
    "    features[:,8]=f9\n",
    "    \n",
    "    labels = ['mean','variance','lumpiness','level shift mean','level shift variance',\n",
    "              'burstiness','maximum','minimum','hightolowmu']\n",
    "    \n",
    "    \n",
    "    #make diagnostic plot\n",
    "    if (plots != ''):\n",
    "     gs1 = gridspec.GridSpec(nfeatures, 1)\n",
    "     gs1.update(left=0.1, right=0.9, bottom=0.1,top = 0.9, wspace=0.05,hspace = 0.0)\n",
    "     for i in range(nfeatures):\n",
    "      ax1 = plt.subplot(gs1[i, 0])\n",
    "      ax1.plot(features[:,i],label=labels[i])\n",
    "      #ax1.set_ylabel(labels[i])\n",
    "      if (i == nfeatures - 1):\n",
    "       ax1.set_xlabel('Time series ID')\n",
    "      ax1.text(1.1,0.5,labels[i],ha='left',transform=ax1.transAxes)\n",
    "      if (plots == 'show'):\n",
    "       plt.show()\n",
    "      else:\n",
    "       plt.savefig(plots)\n",
    "     \n",
    "    return(features,labels)\n",
    "\n",
    "\n",
    "#define the evd function. 2 modes, if training no need to specify pca_inp, enter training data. \n",
    "#The feature space will be calculated on the training data and PCA used to transform this\n",
    "#INPUT: data_y 2D array nepochs x ntime series\n",
    "#       clevel The desired confidence level to consider new points outliers\n",
    "#OUTPUT: xkde,ykde = np.array(nres) The x and y values of the kernal density estimation\n",
    "#        zkde = np.array((nres,nres)) The kde estimated from the training data\n",
    "\n",
    "#If testing new data, must also input the pca_inp (output of sklearn pca routine).\n",
    "#new data will be transformed into the PCA space and their kde's estimated\n",
    "#INPUT: data_y 2D array of the TEST data\n",
    "#       xkde,ykde = np.array(nres) The x and y values of the kernal density estimation\n",
    "#        zkde = np.array((nres,nres)) The kde estimated from the training data\n",
    "#       clevel the confidence level to consider new points outliers\n",
    "from sklearn.decomposition import PCA\n",
    "def evd(data_test,data_train,clevel=0.9973,plots='show'):\n",
    " \n",
    "\n",
    "    \n",
    " op_test = features(data_test,window=10,plots=plots)\n",
    " op_train = features(data_train,window=10,plots=plots)\n",
    " #normalise the feature matrix\n",
    " f_train,flab_train = op_train\n",
    " f_test,flab_test = op_test\n",
    " \n",
    " nts_train,nfeatures = np.shape(f_train)\n",
    " nts_test,nfeatures = np.shape(f_test)\n",
    " fop_train = np.array(f_train)\n",
    " fop_test = np.array(f_test)\n",
    " for i in range(nfeatures):\n",
    "     fop_train[:,i] = (f_train[:,i] - np.mean(f_train[:,i]))/np.std(f_train[:,i])  \n",
    "     fop_test[:,i] = (f_test[:,i] - np.mean(f_test[:,i]))/np.std(f_test[:,i])  \n",
    " \n",
    " #perform PCA\n",
    " pca = PCA(n_components=2)\n",
    " pcafit = pca.fit(fop_train)\n",
    " fop_pca_train = pcafit.fit_transform(fop_train)\n",
    " fop_pca_test = pcafit.fit_transform(fop_test)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " #fit 2d KDE to data\n",
    " from scipy import stats\n",
    " xmin,xmax = min(np.min(fop_pca_train[:,0]),np.min(fop_pca_test[:,0])),max(np.max(fop_pca_train[:,0]),np.max(fop_pca_test[:,0]))\n",
    " ymin,ymax = min(np.min(fop_pca_train[:,1]),np.min(fop_pca_test[:,1])),max(np.max(fop_pca_train[:,1]),np.max(fop_pca_test[:,1]))\n",
    "\n",
    " X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    " positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    " xmod = X[:,0]\n",
    " ymod = Y[0,:]\n",
    " nxmod = np.shape(xmod)[0]\n",
    "\n",
    " values = fop_pca_train.T \n",
    " kernel = stats.gaussian_kde(values)\n",
    " Z = np.reshape(kernel(positions).T, X.shape) \n",
    " clevels = kdestats.confmap(Z.T, [clevel])\n",
    " \n",
    " #make a scatter plot\n",
    " if (plots != ''):\n",
    "  fig = plt.figure()\n",
    "  ax1 = fig.add_subplot(111)\n",
    "  ax1.scatter(fop_pca_train[:,0],fop_pca_train[:,1])\n",
    "  ax1.set_xlabel('PCA 1')\n",
    "  ax1.set_ylabel('PCA 2')\n",
    "  ax1.set_title('PCA Eigen Vector Plot (train mode)')\n",
    "  ax1.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,extent=[xmin, xmax, ymin, ymax])\n",
    "  if (plots == 'show'):\n",
    "   plt.show()\n",
    "  else:\n",
    "   plt.savefig(plots)\n",
    " \n",
    "    \n",
    " ntrain = np.shape(fop_pca_test)[0]\n",
    " idx_x = np.array(np.interp(fop_pca_test[:,0],xmod,np.arange(nxmod)),dtype='int')\n",
    " idx_y = np.array(np.interp(fop_pca_test[:,1],ymod,np.arange(nxmod)),dtype='int') \n",
    " zdat = Z[idx_x,idx_y]\n",
    " for i in range(len(clevels)):\n",
    "  idgood = np.where(zdat > clevels[i])[0]\n",
    "  idbad = np.where(zdat < clevels[i])[0]\n",
    "  \n",
    "  #make plot of contour levels showing good and bad time series\n",
    "  if (plots != ''):\n",
    "   fig = plt.figure()\n",
    "   ax1 = fig.add_subplot(111)\n",
    "   ax1.scatter(fop_pca_train[:,0],fop_pca_train[:,1],label='training points')\n",
    "   ax1.set_xlabel('PCA 1')\n",
    "   ax1.set_ylabel('PCA 2')\n",
    "   ax1.set_title('PCA Eigen Vector Plot (test mode)')\n",
    "   ax1.contour(X,Y, Z, clevels)\n",
    "   ax1.scatter(fop_pca_test[idbad,0],fop_pca_test[idbad,1],label='outliers test data',color='r')\n",
    "   ax1.scatter(fop_pca_test[idgood,0],fop_pca_test[idgood,1],label='good points test data',color='black')\n",
    "   ax1.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,extent=[xmin, xmax, ymin, ymax])\n",
    "   plt.legend()\n",
    "   if (plots == 'show'):\n",
    "    plt.show()\n",
    "   else:\n",
    "    plt.savefig(plots)\n",
    "    \n",
    "  #plot the time series in an imshow flagging up outlying series\n",
    "  if (plots != ''):\n",
    "   fig = plt.figure()\n",
    "   ax1 = fig.add_subplot(111)\n",
    "   dis = data_test.T\n",
    "   dis[idbad,:] = np.nan\n",
    "   col = ax1.imshow(dis,aspect='auto')\n",
    "   plt.colorbar(col,label='Time series value')\n",
    "   ax1.set_xlabel('Time')\n",
    "   ax1.set_ylabel('Time series ID')\n",
    "   if (plots == 'show'):\n",
    "    plt.show()\n",
    "   else:\n",
    "    plt.savefig(plots)\n",
    "\n",
    "  return(zdat,Z,idx_x,idx_y,idbad,fop_pca_train,fop_pca_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_evd = 0\n",
    "if (demo_evd == 1):\n",
    " #introduce some time-varying noise into the model\n",
    " sd_ts = 4.0\n",
    " sd_epoch = 50.0\n",
    " amp_ts_max = 100\n",
    " amp_epoch = 100.0\n",
    " mean_ts = 53.0\n",
    " mean_epoch = 600.0\n",
    " \n",
    " \n",
    " \n",
    " nts = 100\n",
    " nepoch = 1000\n",
    " dat = np.random.randn(nepoch*nts).reshape(nepoch,nts)\n",
    " dat_train = np.array(dat)\n",
    " for i in range(nts):\n",
    "  amp_ts   = amp_ts_max * np.exp(-0.5*((i*1. - mean_ts)/sd_ts)**2)\n",
    "  dat[:,i] = dat[:,i] + amp_ts* np.exp(-0.5*((np.arange(nepoch) - mean_epoch)/sd_epoch)**2) \n",
    "  \n",
    "  \n",
    " plt.clf()\n",
    " fig = plt.figure()\n",
    " ax1 = fig.add_subplot(111)\n",
    " a = ax1.imshow(dat.T,aspect = 'auto')\n",
    " plt.colorbar(a,label='Time series value')\n",
    " ax1.set_xlabel('Time')\n",
    " ax1.set_ylabel('Time series ID')\n",
    " plt.show()\n",
    " \n",
    " zdat,Z,idx_x,idx_y,idbad,fop_pca_train,fop_pca_test = evd(dat,dat_train,clevel=0.9973)\n",
    " \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Non-stationarity}\n",
    "It is relatively easy to update this model to allow for non-stationarity. We just define a window of length 'w', input the most recent w epochs of known normal behaviour (a training data set) and the most recent w epochs. Using KDE, if > 0.5 the new observsations lie outside the confidence region of the training data, we update the KDE contours as the new 'normal behaviour'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os \n",
    "\n",
    "def evd_nonstat(train_new,test_new,clevel=0.9973,frac_check=0.5):\n",
    "    \n",
    " \n",
    " zdat,Z,idx_x,idx_y,idbad,fop_pca_train,fop_pca_test = \\\n",
    " evd(test_new,train_new,clevel=clevel,plots='show')\n",
    " \n",
    " nwindow,nts = np.shape(test_new)\n",
    " frac = 1.*np.shape(idbad)[0]/nwindow\n",
    " if (frac > frac_check):\n",
    "  print 'new \"normal\" behaviour detected. All good'\n",
    "  idbad_out = []\n",
    "  newnorm = 1\n",
    " else:\n",
    "  idbad_out = idx_y\n",
    "  newnorm = 0\n",
    " \n",
    " return(idbad_out,newnorm,fop_pca_train,fop_pca_test)\n",
    "\n",
    "#perform the above routine over a whole time series retiurning the indicees of bad combinations\n",
    "#of time series and epoch id_out[epoch,ts]\n",
    "def evd_evolve(train,test,window=20,clevel=0.9973,diagplots=1):\n",
    " nepoch,nts = np.shape(test)\n",
    " ilo = 0\n",
    " train_in = train#train[ilo:window,:]\n",
    " id_out = np.zeros((0,2))\n",
    " for i in range(nepoch-window):\n",
    "  test_in  = test[i:i+window,:]\n",
    "  print np.shape(train_in),np.shape(test_in)\n",
    "  idbad_out,newnorm,fop_pca_train,fop_pca_test = evd_nonstat(train_in,test_in,clevel=0.9973,frac_check=0.5) \n",
    "  idbad_out = np.array(idbad_out)\n",
    "  print 'herer',i\n",
    "  if (newnorm == 1): \n",
    "   train_in = test_in\n",
    "   print 'new behavour',i\n",
    "  else:\n",
    "   nbad = np.shape(idbad_out)[0]\n",
    "   idnow = np.ones(nbad)*i\n",
    "   idn = np.zeros((nbad,2))\n",
    "   idn[:,0]=idnow\n",
    "   idn[:,1]=np.array(idbad_out)\n",
    "   id_out = np.vstack((id_out,idn))\n",
    "   \n",
    "   if (diagplots == 1):\n",
    "    if (i == 0):\n",
    "     os.system('mkdir ./diagplots_evdevolve')\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.scatter(fop_pca_train[:,0],fop_pca_train[:,1],label='train')\n",
    "    ax1.scatter(fop_pca_test[:,0],fop_pca_test[:,1],label='test good')\n",
    "    ax1.scatter(fop_pca_test[idbad_out,0],fop_pca_test[idbad_out,1],label='test bad')\n",
    "  \n",
    "    #plot confidence kernel\n",
    "    xmin,xmax = min(np.min(fop_pca_train[:,0]),np.min(fop_pca_test[:,0])),max(np.max(fop_pca_train[:,0]),np.max(fop_pca_test[:,0]))\n",
    "    ymin,ymax = min(np.min(fop_pca_train[:,1]),np.min(fop_pca_test[:,1])),max(np.max(fop_pca_train[:,1]),np.max(fop_pca_test[:,1]))\n",
    "    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    xmod = X[:,0]\n",
    "    ymod = Y[0,:]\n",
    "    nxmod = np.shape(xmod)[0]\n",
    "    values = fop_pca_train.T \n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    Z = np.reshape(kernel(positions).T, X.shape) \n",
    "    clevels = kdestats.confmap(Z.T, [clevel])\n",
    "    ax1.contour(X,Y, Z, clevels)\n",
    "    \n",
    "    ax1.set_title('time '+np.str(i))\n",
    "    ax1.legend()\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    tnew = np.array(test)\n",
    "    tnew[i:i+window,idbad_out] = np.nan\n",
    "    ax2.imshow(tnew.T,aspect='auto')\n",
    "    xl = list(ax2.get_xlim())\n",
    "    yl = list(ax2.get_ylim())\n",
    "    ax2.plot([i]*2,yl,color='r')\n",
    "    ax2.plot([i+window]*2,yl,color='r')\n",
    "    plt.savefig('./diagplots_evdevolve/fig_evde_'+\"%04d\" % i +'.pdf')\n",
    "\n",
    " id_out = np.array(id_out,dtype='int')\n",
    "    \n",
    " \n",
    " return(id_out)\n",
    "    \n",
    "demo_nonstat = 1\n",
    "if (demo_nonstat == 1):\n",
    " #introduce some time-varying noise into the model\n",
    " sd_ts = 4.0\n",
    " sd_epoch = 50.0\n",
    " amp_ts_max = 100\n",
    " amp_epoch = 100.0\n",
    " mean_ts = 53.0\n",
    " mean_epoch = 600.0\n",
    " \n",
    " \n",
    " \n",
    " nts = 100\n",
    " nepoch = 1000\n",
    " dat = np.random.randn(nepoch*nts).reshape(nepoch,nts)\n",
    " dat_train = np.array(dat)\n",
    " for i in range(nts):\n",
    "  amp_ts   = amp_ts_max * np.exp(-0.5*((i*1. - mean_ts)/sd_ts)**2)\n",
    "  for i2 in range(nepoch):\n",
    "   if (i2 > mean_epoch):\n",
    "    dat[i2,i] = dat[i2,i] + amp_ts\n",
    "   else:\n",
    "    dat[i2,i] = dat[i2,i] + amp_ts*np.exp(-0.5*((i2 - mean_epoch)/sd_epoch)**2) \n",
    "  \n",
    " plt.clf()\n",
    " fig = plt.figure()\n",
    " ax1 = fig.add_subplot(111)\n",
    " a = ax1.imshow(dat.T,aspect = 'auto')\n",
    " plt.colorbar(a,label='Time series value')\n",
    " ax1.set_xlabel('Time')\n",
    " ax1.set_ylabel('Time series ID')\n",
    " plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #now test the function\n",
    " id = evd_evolve(dat_train,dat,window=20,clevel=0.9973)\n",
    " id = np.array(id,dtype='int')\n",
    " dat.T[id[:,0],id[:,1]]=np.nan\n",
    " plt.clf()\n",
    " fig = plt.figure()\n",
    " ax1 = fig.add_subplot(111)\n",
    " a = ax1.imshow(dat.T,aspect = 'auto')\n",
    " plt.colorbar(a,label='Time series value post nan')\n",
    " ax1.set_xlabel('Time')\n",
    " ax1.set_ylabel('Time series ID')\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id = np.array(id,dtype='int')\n",
    "dat[id[:,0],id[:,1]]=np.nan\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "a = ax1.imshow(dat.T,aspect = 'auto')\n",
    "plt.colorbar(a,label='Time series value post nan')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Time series ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function to tie everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUTS\n",
    "#data_y enter a 1D array for single-time series anomaly detection to identify points or clusters of points\n",
    "#from within a single time series, can enter a 2D array to batch run lots of time series in a single function call\n",
    "\n",
    "#data_y enter a 2D array and set runtype 'parallel' to compute outliers at each epoch by comparing \n",
    "#with adjacent time series at the same epoch\n",
    "\n",
    "#data_y enter a list with 2 elements, the first is a 2D array of training data n_epochs x n_timeseries, the 2nd is a\n",
    "#2D array of test data. This setting will use the Extreme value distribution method of Talagala et al 2018\n",
    "#to identify entire time series that exhibit anaomalies. Useful for corelated anomalies that arrise slowly\n",
    "#and fool other outlier-detection methods.\n",
    "\n",
    "#spread_function: for series modes, outliers are identified that are sd_check*spread_function from the model\n",
    "# function specified in fname\n",
    "\n",
    "\n",
    "def outlier_smooth(data_y,sd_check=5,fname='running median',runtype='series',filter_size = 5,\n",
    "                   max_iteration=10,diagnostic_figure='',spread_function = 'rms'):\n",
    "\n",
    " if (runtype=='parallel'):\n",
    "  if (type(data_y)==np.ndarray):\n",
    "   if (len(np.shape(data_y)) !=2):\n",
    "    raise ValueError('For parallel outlier detection, \\\n",
    "    data_y must be a 2D numpy array with epochs as the first axis and \\\n",
    "    different light curves as the second')\n",
    "  id_outliers = outrej_parallel(data_y,sd_check=sd_check,fname=fname,filter_size = filter_size,\n",
    "                                max_iteration=max_iteration,diagnostic_figure=diagnostic_figure,\n",
    "                               spread_function = spread_function)\n",
    " \n",
    " #if in parallel and want to use the EVD analysis method from Talgala et al 2018, enter a list for data_y \n",
    " #containing the test and training data. id_outliers will now return a 1d array corresponding to the \n",
    " #outlying time series\n",
    " elif (type(data_y)==list): \n",
    "  zdat,Z,idx_x,idx_y,idbad,fop_pca_train,fop_pca_test = evd(data_y[1],data_y[0],clevel=0.9973)\n",
    "  id_outliers = idbad\n",
    "    \n",
    " elif (runtype == 'series'):\n",
    "  if (len(np.shape(data_y)) ==1):\n",
    "    a,b,c,id_outliers = outrej(data_y,sd_check=sd_check,fname=fname,\n",
    "                               filter_size = filter_size,max_iteration = max_iteration,\n",
    "                               diagnostic_figure=diagnostic_figure,comments=0,spread_function = spread_function)\n",
    "  else:\n",
    "   id_outliers = outrej_series(data_y,sd_check=sd_check,fname=fname,filter_size = filter_size,\n",
    "                               max_iteration=max_iteration,diagnostic_figure=diagnostic_figure,\n",
    "                              spread_function = spread_function) \n",
    "\n",
    " else:\n",
    "  raise ValueError('Please ensure that the \"runtype\" argument is specified as either parallel or series and \\\\\n",
    "  that the input data is in the form of either \\n \\\n",
    "  1) A 1D numpy array for a single timeseries with epochs along the axis. or \\n \\\n",
    "  2) A 2D numpy array for a data cube of multiple time series with epochs as the first axis and \\\n",
    "     different time series as the second.')\n",
    " \n",
    " return(id_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
